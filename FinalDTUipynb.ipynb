{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalDTUipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xzZ8P29aalj",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crKiThzieR49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjVnZTgQfhwV",
        "colab_type": "text"
      },
      "source": [
        "**Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acjr43KLx9in",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "62af78f0-0728-42ac-af34-4e7749e5c499"
      },
      "source": [
        "vp = pd.read_csv('/content/vp.csv')\n",
        "vp.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Roll No.</th>\n",
              "      <th>Math1</th>\n",
              "      <th>Physics1</th>\n",
              "      <th>EE</th>\n",
              "      <th>CO</th>\n",
              "      <th>ED</th>\n",
              "      <th>FEC1</th>\n",
              "      <th>SGPGA1</th>\n",
              "      <th>Math2</th>\n",
              "      <th>Physics2</th>\n",
              "      <th>Chem</th>\n",
              "      <th>BME</th>\n",
              "      <th>Workshop</th>\n",
              "      <th>FEC2</th>\n",
              "      <th>SGPGA2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1    VASU GUPTA</td>\n",
              "      <td>2017/B1/67</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>9.5</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>9.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2    TANISH GROVER</td>\n",
              "      <td>2017/B1/68</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>9.4</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>9.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3    DHANANJAY NEGI</td>\n",
              "      <td>2017/B1/69</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7.7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>7.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4    AMIT KUMAR SINGH</td>\n",
              "      <td>2017/B1/70</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8.5</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>9.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5    RITIK AGGARWAL</td>\n",
              "      <td>2017/B1/71</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>8.8</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Name    Roll No.  Math1  ...  Workshop  FEC2  SGPGA2\n",
              "0        1    VASU GUPTA  2017/B1/67      7  ...         7     5     9.8\n",
              "1     2    TANISH GROVER  2017/B1/68      7  ...         4     6     9.2\n",
              "2    3    DHANANJAY NEGI  2017/B1/69      6  ...         6     4     7.8\n",
              "3  4    AMIT KUMAR SINGH  2017/B1/70      5  ...         7     5     9.8\n",
              "4    5    RITIK AGGARWAL  2017/B1/71      6  ...         6     5     9.5\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PniJoCJnfvHB",
        "colab_type": "text"
      },
      "source": [
        "**Functions to be used** : **plot_graphs** : used to plot graphs for epochs vs loss, \n",
        "                            **NNpredict** : used to create and train a neural 4 layered neural network(64,32,16,1 nodes respectively),\n",
        "                            **to_arr** :used to convert grades into array with numbers\n",
        "                            "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhNEbUpFL_5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "def NNpredict(X_train,X_test,y_train,y_test):\n",
        "  model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Dense(64, input_shape = [6], activation = 'relu'),\n",
        "                            tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "                            tf.keras.layers.Dense(16, activation = 'relu'),\n",
        "                            tf.keras.layers.Dense(1)])\n",
        "  opt = tf.keras.optimizers.Adam(0.001)\n",
        "  model.compile(loss = 'mse', optimizer = opt)\n",
        "  history = model.fit(X_train,y_train, batch_size=X_train.shape[0],epochs=300, \n",
        "                    #callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_scheduler)],\n",
        "                    validation_data=(X_test,y_test))\n",
        "  return model, history, sklearn.metrics.r2_score(y_test,model.predict(X_test))\n",
        "def to_arr(stri):\n",
        "  grade2idx = {'F':0,'P':1,'C':2,'B':3,'B+':4,'A':5,'A+':6,'O':7}\n",
        "  temp = stri.split(' ')\n",
        "  temp = [grade2idx[i] for i in temp]\n",
        "  return np.array(temp)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvcE9a0XPV7k",
        "colab_type": "text"
      },
      "source": [
        "### Model for sem1 B batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gu97_jD0zE7s",
        "colab": {}
      },
      "source": [
        "X=vp[['Math1', 'Physics1', 'EE', 'CO', 'ED', 'FEC1']] # taking the semester 1 subjects for B batch from our dataset\n",
        "#Y = Final[['Math2', 'Physics2', 'Chem', 'BME', 'Workshop', 'FEC2']] this will be our predicted subjects\n",
        "Y=vp['SGPGA2']\n",
        "#X = X/28\n",
        "# converting everything into array and splitting our data into train and test sets\n",
        "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X,Y,test_size=0.3,random_state=101)\n",
        "X_trainB = np.array(X_trainB)\n",
        "X_testB = np.array(X_testB)\n",
        "y_trainB = np.array(y_trainB)\n",
        "y_testB = np.array(y_testB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fgFz3sfwrSF",
        "colab_type": "text"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLimtRw025el",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa6ef1e6-4733-400f-cf51-92042a114925"
      },
      "source": [
        "m,h,r = NNpredict(X_trainB,X_testB,y_trainB,y_testB) # training neural network\n",
        "plot_graphs(h,'loss') # plotting loss function\n",
        "print(r) # printing r2 accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 54.6519 - val_loss: 50.0554\n",
            "Epoch 2/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 49.3107 - val_loss: 44.8371\n",
            "Epoch 3/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 44.2498 - val_loss: 39.9789\n",
            "Epoch 4/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 39.5307 - val_loss: 35.5589\n",
            "Epoch 5/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 35.2658 - val_loss: 31.6816\n",
            "Epoch 6/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 31.5522 - val_loss: 28.4428\n",
            "Epoch 7/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 28.4427 - val_loss: 25.7936\n",
            "Epoch 8/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 25.8690 - val_loss: 23.5093\n",
            "Epoch 9/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 23.6271 - val_loss: 21.4093\n",
            "Epoch 10/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 21.5442 - val_loss: 19.3704\n",
            "Epoch 11/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 19.5353 - val_loss: 17.3778\n",
            "Epoch 12/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 17.5606 - val_loss: 15.4329\n",
            "Epoch 13/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 15.6308 - val_loss: 13.5575\n",
            "Epoch 14/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 13.7679 - val_loss: 11.7745\n",
            "Epoch 15/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 11.9935 - val_loss: 10.1068\n",
            "Epoch 16/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 10.3333 - val_loss: 8.5723\n",
            "Epoch 17/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8.8036 - val_loss: 7.1871\n",
            "Epoch 18/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7.4197 - val_loss: 5.9669\n",
            "Epoch 19/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 6.1958 - val_loss: 4.9234\n",
            "Epoch 20/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 5.1439 - val_loss: 4.0656\n",
            "Epoch 21/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 4.2729 - val_loss: 3.4002\n",
            "Epoch 22/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 3.5882 - val_loss: 2.9333\n",
            "Epoch 23/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 3.0972 - val_loss: 2.6665\n",
            "Epoch 24/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.8018 - val_loss: 2.5904\n",
            "Epoch 25/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.6928 - val_loss: 2.6787\n",
            "Epoch 26/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2.7473 - val_loss: 2.8857\n",
            "Epoch 27/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 2.9203 - val_loss: 3.1469\n",
            "Epoch 28/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 3.1506 - val_loss: 3.4004\n",
            "Epoch 29/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 3.3777 - val_loss: 3.5964\n",
            "Epoch 30/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 3.5546 - val_loss: 3.7076\n",
            "Epoch 31/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 3.6541 - val_loss: 3.7279\n",
            "Epoch 32/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 3.6699 - val_loss: 3.6683\n",
            "Epoch 33/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 3.6109 - val_loss: 3.5503\n",
            "Epoch 34/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 3.4952 - val_loss: 3.3936\n",
            "Epoch 35/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 3.3460 - val_loss: 3.2190\n",
            "Epoch 36/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 3.1826 - val_loss: 3.0454\n",
            "Epoch 37/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 3.0221 - val_loss: 2.8836\n",
            "Epoch 38/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.8746 - val_loss: 2.7395\n",
            "Epoch 39/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 2.7455 - val_loss: 2.6160\n",
            "Epoch 40/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.6364 - val_loss: 2.5141\n",
            "Epoch 41/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.5479 - val_loss: 2.4343\n",
            "Epoch 42/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.4801 - val_loss: 2.3745\n",
            "Epoch 43/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2.4311 - val_loss: 2.3321\n",
            "Epoch 44/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.3984 - val_loss: 2.3039\n",
            "Epoch 45/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.3784 - val_loss: 2.2869\n",
            "Epoch 46/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.3680 - val_loss: 2.2774\n",
            "Epoch 47/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.3635 - val_loss: 2.2723\n",
            "Epoch 48/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.3620 - val_loss: 2.2684\n",
            "Epoch 49/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 2.3609 - val_loss: 2.2635\n",
            "Epoch 50/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2.3576 - val_loss: 2.2561\n",
            "Epoch 51/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.3506 - val_loss: 2.2453\n",
            "Epoch 52/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2.3390 - val_loss: 2.2300\n",
            "Epoch 53/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 2.3222 - val_loss: 2.2103\n",
            "Epoch 54/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2.3001 - val_loss: 2.1870\n",
            "Epoch 55/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2.2731 - val_loss: 2.1604\n",
            "Epoch 56/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.2425 - val_loss: 2.1316\n",
            "Epoch 57/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.2090 - val_loss: 2.1021\n",
            "Epoch 58/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 2.1747 - val_loss: 2.0739\n",
            "Epoch 59/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.1407 - val_loss: 2.0481\n",
            "Epoch 60/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.1087 - val_loss: 2.0263\n",
            "Epoch 61/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.0804 - val_loss: 2.0095\n",
            "Epoch 62/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.0567 - val_loss: 1.9989\n",
            "Epoch 63/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2.0389 - val_loss: 1.9938\n",
            "Epoch 64/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.0272 - val_loss: 1.9937\n",
            "Epoch 65/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.0211 - val_loss: 1.9958\n",
            "Epoch 66/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.0180 - val_loss: 1.9972\n",
            "Epoch 67/300\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 2.0155 - val_loss: 1.9955\n",
            "Epoch 68/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 2.0109 - val_loss: 1.9887\n",
            "Epoch 69/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.0022 - val_loss: 1.9766\n",
            "Epoch 70/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.9891 - val_loss: 1.9600\n",
            "Epoch 71/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.9723 - val_loss: 1.9408\n",
            "Epoch 72/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.9533 - val_loss: 1.9208\n",
            "Epoch 73/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.9339 - val_loss: 1.9021\n",
            "Epoch 74/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.9157 - val_loss: 1.8857\n",
            "Epoch 75/300\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.8998 - val_loss: 1.8723\n",
            "Epoch 76/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.8864 - val_loss: 1.8612\n",
            "Epoch 77/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.8750 - val_loss: 1.8516\n",
            "Epoch 78/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 1.8648 - val_loss: 1.8431\n",
            "Epoch 79/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.8552 - val_loss: 1.8355\n",
            "Epoch 80/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.8458 - val_loss: 1.8277\n",
            "Epoch 81/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.8360 - val_loss: 1.8195\n",
            "Epoch 82/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.8257 - val_loss: 1.8109\n",
            "Epoch 83/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.8159 - val_loss: 1.8031\n",
            "Epoch 84/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.8068 - val_loss: 1.7966\n",
            "Epoch 85/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.7990 - val_loss: 1.7915\n",
            "Epoch 86/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.7927 - val_loss: 1.7875\n",
            "Epoch 87/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.7871 - val_loss: 1.7839\n",
            "Epoch 88/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.7820 - val_loss: 1.7796\n",
            "Epoch 89/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.7765 - val_loss: 1.7744\n",
            "Epoch 90/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.7699 - val_loss: 1.7680\n",
            "Epoch 91/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.7622 - val_loss: 1.7609\n",
            "Epoch 92/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.7537 - val_loss: 1.7544\n",
            "Epoch 93/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.7455 - val_loss: 1.7487\n",
            "Epoch 94/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.7379 - val_loss: 1.7438\n",
            "Epoch 95/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.7309 - val_loss: 1.7397\n",
            "Epoch 96/300\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.7244 - val_loss: 1.7358\n",
            "Epoch 97/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.7182 - val_loss: 1.7321\n",
            "Epoch 98/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.7120 - val_loss: 1.7285\n",
            "Epoch 99/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.7059 - val_loss: 1.7254\n",
            "Epoch 100/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.7007 - val_loss: 1.7233\n",
            "Epoch 101/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.6958 - val_loss: 1.7213\n",
            "Epoch 102/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.6910 - val_loss: 1.7193\n",
            "Epoch 103/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.6863 - val_loss: 1.7166\n",
            "Epoch 104/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.6813 - val_loss: 1.7132\n",
            "Epoch 105/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.6760 - val_loss: 1.7094\n",
            "Epoch 106/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.6705 - val_loss: 1.7054\n",
            "Epoch 107/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.6651 - val_loss: 1.7015\n",
            "Epoch 108/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.6600 - val_loss: 1.6974\n",
            "Epoch 109/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.6550 - val_loss: 1.6934\n",
            "Epoch 110/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.6500 - val_loss: 1.6895\n",
            "Epoch 111/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.6448 - val_loss: 1.6856\n",
            "Epoch 112/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.6396 - val_loss: 1.6815\n",
            "Epoch 113/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.6345 - val_loss: 1.6775\n",
            "Epoch 114/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.6296 - val_loss: 1.6735\n",
            "Epoch 115/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.6248 - val_loss: 1.6695\n",
            "Epoch 116/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.6201 - val_loss: 1.6653\n",
            "Epoch 117/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.6153 - val_loss: 1.6608\n",
            "Epoch 118/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.6104 - val_loss: 1.6561\n",
            "Epoch 119/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.6058 - val_loss: 1.6517\n",
            "Epoch 120/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.6014 - val_loss: 1.6476\n",
            "Epoch 121/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.5971 - val_loss: 1.6438\n",
            "Epoch 122/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.5928 - val_loss: 1.6402\n",
            "Epoch 123/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.5886 - val_loss: 1.6368\n",
            "Epoch 124/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.5845 - val_loss: 1.6334\n",
            "Epoch 125/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.5804 - val_loss: 1.6302\n",
            "Epoch 126/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.5762 - val_loss: 1.6271\n",
            "Epoch 127/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 1.5720 - val_loss: 1.6238\n",
            "Epoch 128/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.5675 - val_loss: 1.6206\n",
            "Epoch 129/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.5631 - val_loss: 1.6175\n",
            "Epoch 130/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.5587 - val_loss: 1.6145\n",
            "Epoch 131/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.5543 - val_loss: 1.6115\n",
            "Epoch 132/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.5500 - val_loss: 1.6082\n",
            "Epoch 133/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.5456 - val_loss: 1.6047\n",
            "Epoch 134/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.5411 - val_loss: 1.6009\n",
            "Epoch 135/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.5366 - val_loss: 1.5968\n",
            "Epoch 136/300\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.5321 - val_loss: 1.5927\n",
            "Epoch 137/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.5276 - val_loss: 1.5882\n",
            "Epoch 138/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.5230 - val_loss: 1.5837\n",
            "Epoch 139/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.5185 - val_loss: 1.5794\n",
            "Epoch 140/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.5141 - val_loss: 1.5749\n",
            "Epoch 141/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.5097 - val_loss: 1.5703\n",
            "Epoch 142/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.5053 - val_loss: 1.5655\n",
            "Epoch 143/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.5009 - val_loss: 1.5608\n",
            "Epoch 144/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.4965 - val_loss: 1.5564\n",
            "Epoch 145/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.4922 - val_loss: 1.5523\n",
            "Epoch 146/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.4879 - val_loss: 1.5485\n",
            "Epoch 147/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.4835 - val_loss: 1.5446\n",
            "Epoch 148/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.4791 - val_loss: 1.5405\n",
            "Epoch 149/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.4746 - val_loss: 1.5364\n",
            "Epoch 150/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.4701 - val_loss: 1.5323\n",
            "Epoch 151/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.4656 - val_loss: 1.5283\n",
            "Epoch 152/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.4610 - val_loss: 1.5242\n",
            "Epoch 153/300\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.4566 - val_loss: 1.5199\n",
            "Epoch 154/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.4521 - val_loss: 1.5155\n",
            "Epoch 155/300\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.4476 - val_loss: 1.5108\n",
            "Epoch 156/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.4429 - val_loss: 1.5062\n",
            "Epoch 157/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.4382 - val_loss: 1.5015\n",
            "Epoch 158/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.4334 - val_loss: 1.4969\n",
            "Epoch 159/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.4285 - val_loss: 1.4922\n",
            "Epoch 160/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.4236 - val_loss: 1.4873\n",
            "Epoch 161/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.4187 - val_loss: 1.4822\n",
            "Epoch 162/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.4137 - val_loss: 1.4771\n",
            "Epoch 163/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.4086 - val_loss: 1.4719\n",
            "Epoch 164/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.4034 - val_loss: 1.4665\n",
            "Epoch 165/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 1.3980 - val_loss: 1.4610\n",
            "Epoch 166/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.3925 - val_loss: 1.4552\n",
            "Epoch 167/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.3868 - val_loss: 1.4491\n",
            "Epoch 168/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.3811 - val_loss: 1.4431\n",
            "Epoch 169/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.3750 - val_loss: 1.4368\n",
            "Epoch 170/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.3683 - val_loss: 1.4300\n",
            "Epoch 171/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.3608 - val_loss: 1.4225\n",
            "Epoch 172/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.3524 - val_loss: 1.4142\n",
            "Epoch 173/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.3428 - val_loss: 1.4056\n",
            "Epoch 174/300\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.3344 - val_loss: 1.3972\n",
            "Epoch 175/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.3274 - val_loss: 1.3903\n",
            "Epoch 176/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.3219 - val_loss: 1.3830\n",
            "Epoch 177/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.3157 - val_loss: 1.3768\n",
            "Epoch 178/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.3104 - val_loss: 1.3696\n",
            "Epoch 179/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.3043 - val_loss: 1.3630\n",
            "Epoch 180/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2986 - val_loss: 1.3565\n",
            "Epoch 181/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2922 - val_loss: 1.3506\n",
            "Epoch 182/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.2863 - val_loss: 1.3437\n",
            "Epoch 183/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.2798 - val_loss: 1.3370\n",
            "Epoch 184/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.2738 - val_loss: 1.3300\n",
            "Epoch 185/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2672 - val_loss: 1.3238\n",
            "Epoch 186/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.2611 - val_loss: 1.3172\n",
            "Epoch 187/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2545 - val_loss: 1.3114\n",
            "Epoch 188/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.2484 - val_loss: 1.3060\n",
            "Epoch 189/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2423 - val_loss: 1.3014\n",
            "Epoch 190/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.2370 - val_loss: 1.2965\n",
            "Epoch 191/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2314 - val_loss: 1.2925\n",
            "Epoch 192/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2264 - val_loss: 1.2885\n",
            "Epoch 193/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.2212 - val_loss: 1.2844\n",
            "Epoch 194/300\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.2163 - val_loss: 1.2801\n",
            "Epoch 195/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2115 - val_loss: 1.2758\n",
            "Epoch 196/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.2064 - val_loss: 1.2717\n",
            "Epoch 197/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.2017 - val_loss: 1.2666\n",
            "Epoch 198/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.1966 - val_loss: 1.2618\n",
            "Epoch 199/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.1919 - val_loss: 1.2571\n",
            "Epoch 200/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.1873 - val_loss: 1.2518\n",
            "Epoch 201/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.1825 - val_loss: 1.2469\n",
            "Epoch 202/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.1782 - val_loss: 1.2419\n",
            "Epoch 203/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.1735 - val_loss: 1.2368\n",
            "Epoch 204/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.1690 - val_loss: 1.2316\n",
            "Epoch 205/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.1646 - val_loss: 1.2267\n",
            "Epoch 206/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.1601 - val_loss: 1.2220\n",
            "Epoch 207/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.1558 - val_loss: 1.2172\n",
            "Epoch 208/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.1517 - val_loss: 1.2126\n",
            "Epoch 209/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.1474 - val_loss: 1.2081\n",
            "Epoch 210/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.1432 - val_loss: 1.2036\n",
            "Epoch 211/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.1391 - val_loss: 1.1993\n",
            "Epoch 212/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.1349 - val_loss: 1.1951\n",
            "Epoch 213/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.1309 - val_loss: 1.1909\n",
            "Epoch 214/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.1270 - val_loss: 1.1869\n",
            "Epoch 215/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.1230 - val_loss: 1.1829\n",
            "Epoch 216/300\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.1192 - val_loss: 1.1791\n",
            "Epoch 217/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.1155 - val_loss: 1.1755\n",
            "Epoch 218/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.1119 - val_loss: 1.1715\n",
            "Epoch 219/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.1082 - val_loss: 1.1677\n",
            "Epoch 220/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.1045 - val_loss: 1.1638\n",
            "Epoch 221/300\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.1010 - val_loss: 1.1601\n",
            "Epoch 222/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0976 - val_loss: 1.1565\n",
            "Epoch 223/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.0942 - val_loss: 1.1530\n",
            "Epoch 224/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0910 - val_loss: 1.1496\n",
            "Epoch 225/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.0877 - val_loss: 1.1462\n",
            "Epoch 226/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0846 - val_loss: 1.1428\n",
            "Epoch 227/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0815 - val_loss: 1.1396\n",
            "Epoch 228/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0785 - val_loss: 1.1363\n",
            "Epoch 229/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0756 - val_loss: 1.1333\n",
            "Epoch 230/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0729 - val_loss: 1.1301\n",
            "Epoch 231/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0702 - val_loss: 1.1275\n",
            "Epoch 232/300\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.0677 - val_loss: 1.1243\n",
            "Epoch 233/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0650 - val_loss: 1.1211\n",
            "Epoch 234/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0621 - val_loss: 1.1176\n",
            "Epoch 235/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0590 - val_loss: 1.1147\n",
            "Epoch 236/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0565 - val_loss: 1.1124\n",
            "Epoch 237/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0544 - val_loss: 1.1101\n",
            "Epoch 238/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.0522 - val_loss: 1.1076\n",
            "Epoch 239/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0496 - val_loss: 1.1049\n",
            "Epoch 240/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0469 - val_loss: 1.1026\n",
            "Epoch 241/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0445 - val_loss: 1.1005\n",
            "Epoch 242/300\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.0426 - val_loss: 1.0986\n",
            "Epoch 243/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0407 - val_loss: 1.0963\n",
            "Epoch 244/300\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 1.0387 - val_loss: 1.0936\n",
            "Epoch 245/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0360 - val_loss: 1.0908\n",
            "Epoch 246/300\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.0332 - val_loss: 1.0883\n",
            "Epoch 247/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0309 - val_loss: 1.0862\n",
            "Epoch 248/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0290 - val_loss: 1.0846\n",
            "Epoch 249/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.0274 - val_loss: 1.0824\n",
            "Epoch 250/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0253 - val_loss: 1.0802\n",
            "Epoch 251/300\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.0231 - val_loss: 1.0781\n",
            "Epoch 252/300\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 1.0210 - val_loss: 1.0766\n",
            "Epoch 253/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0195 - val_loss: 1.0755\n",
            "Epoch 254/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0179 - val_loss: 1.0738\n",
            "Epoch 255/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0161 - val_loss: 1.0722\n",
            "Epoch 256/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0141 - val_loss: 1.0708\n",
            "Epoch 257/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0124 - val_loss: 1.0695\n",
            "Epoch 258/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.0111 - val_loss: 1.0684\n",
            "Epoch 259/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0096 - val_loss: 1.0665\n",
            "Epoch 260/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0080 - val_loss: 1.0650\n",
            "Epoch 261/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.0064 - val_loss: 1.0640\n",
            "Epoch 262/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0052 - val_loss: 1.0626\n",
            "Epoch 263/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0041 - val_loss: 1.0615\n",
            "Epoch 264/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0027 - val_loss: 1.0598\n",
            "Epoch 265/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.0013 - val_loss: 1.0584\n",
            "Epoch 266/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0001 - val_loss: 1.0572\n",
            "Epoch 267/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9991 - val_loss: 1.0554\n",
            "Epoch 268/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9981 - val_loss: 1.0540\n",
            "Epoch 269/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.9968 - val_loss: 1.0524\n",
            "Epoch 270/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9955 - val_loss: 1.0511\n",
            "Epoch 271/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.9945 - val_loss: 1.0502\n",
            "Epoch 272/300\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.9934 - val_loss: 1.0490\n",
            "Epoch 273/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.9924 - val_loss: 1.0480\n",
            "Epoch 274/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.9913 - val_loss: 1.0467\n",
            "Epoch 275/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9902 - val_loss: 1.0456\n",
            "Epoch 276/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.9893 - val_loss: 1.0449\n",
            "Epoch 277/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9885 - val_loss: 1.0440\n",
            "Epoch 278/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9882 - val_loss: 1.0443\n",
            "Epoch 279/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9879 - val_loss: 1.0430\n",
            "Epoch 280/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9877 - val_loss: 1.0424\n",
            "Epoch 281/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.9862 - val_loss: 1.0400\n",
            "Epoch 282/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.9846 - val_loss: 1.0390\n",
            "Epoch 283/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9835 - val_loss: 1.0388\n",
            "Epoch 284/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9831 - val_loss: 1.0378\n",
            "Epoch 285/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9829 - val_loss: 1.0376\n",
            "Epoch 286/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9821 - val_loss: 1.0359\n",
            "Epoch 287/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9809 - val_loss: 1.0351\n",
            "Epoch 288/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.9800 - val_loss: 1.0351\n",
            "Epoch 289/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9798 - val_loss: 1.0343\n",
            "Epoch 290/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.9796 - val_loss: 1.0337\n",
            "Epoch 291/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9787 - val_loss: 1.0323\n",
            "Epoch 292/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.9776 - val_loss: 1.0316\n",
            "Epoch 293/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9770 - val_loss: 1.0317\n",
            "Epoch 294/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.9768 - val_loss: 1.0307\n",
            "Epoch 295/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9764 - val_loss: 1.0304\n",
            "Epoch 296/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9755 - val_loss: 1.0294\n",
            "Epoch 297/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9747 - val_loss: 1.0290\n",
            "Epoch 298/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.9743 - val_loss: 1.0291\n",
            "Epoch 299/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9740 - val_loss: 1.0282\n",
            "Epoch 300/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.9735 - val_loss: 1.0280\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.4407309880348562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AINwkX5Fmosp",
        "colab_type": "text"
      },
      "source": [
        "**Randmom Forest Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDGY0Ne8e9u_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a0fdc8d5-6056-4425-91fd-03960c85c882"
      },
      "source": [
        "RFmodel = RandomForestRegressor(n_estimators=900) \n",
        "RFmodel.fit(X_trainB,y_trainB)\n",
        "predict = RFmodel.predict(X_testB) # training random forest regressor\n",
        "sklearn.metrics.r2_score(y_testB,RFmodel.predict(X_testB)) # r2 score of random forest regressor\n",
        "### finding out that random forest worked better than neural network ###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6201982923660673"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyclYoQrSIie",
        "colab_type": "text"
      },
      "source": [
        "## Model for sem1A batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iSnXhv67QXRM",
        "colab": {}
      },
      "source": [
        "## now we have to do sem 1 b batch separately because of difference in subjects ##\n",
        "X=vp[['Math1', 'Physics1', 'Chem', 'BME', 'Workshop', 'FEC1']] # taking the semester 1 subjects for A batch from our dataset\n",
        "#Y = Final[['Math2', 'Physics2', 'Chem', 'BME', 'Workshop', 'FEC2']]\n",
        "Y=vp['SGPGA2']\n",
        "# converting everything to arrays and splitting our data into train and test sets\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X,Y,test_size=0.3,random_state=101)\n",
        "X_train2 = np.array(X_train2)\n",
        "X_test2 = np.array(X_test2)\n",
        "y_train2 = np.array(y_train2)\n",
        "y_test2 = np.array(y_test2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioH4yNykooEE",
        "colab_type": "text"
      },
      "source": [
        "**Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2qJhn9a3QvQW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b138fcc-9308-4ace-d610-222d74cc55b6"
      },
      "source": [
        "m,h,r = NNpredict(X_train2,X_test2,y_train2,y_test2) # training neural network\n",
        "plot_graphs(h,'loss') #plotting loss graph\n",
        "print(r) # prining r2 score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 54.0885 - val_loss: 51.3757\n",
            "Epoch 2/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 50.8230 - val_loss: 48.3083\n",
            "Epoch 3/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 47.7506 - val_loss: 45.3975\n",
            "Epoch 4/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 44.8544 - val_loss: 42.6123\n",
            "Epoch 5/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 42.0919 - val_loss: 39.9412\n",
            "Epoch 6/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 39.4462 - val_loss: 37.3660\n",
            "Epoch 7/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 36.9011 - val_loss: 34.8703\n",
            "Epoch 8/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 34.4344 - val_loss: 32.4423\n",
            "Epoch 9/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 32.0334 - val_loss: 30.0715\n",
            "Epoch 10/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 29.6910 - val_loss: 27.7536\n",
            "Epoch 11/300\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 27.4053 - val_loss: 25.4920\n",
            "Epoch 12/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 25.1738 - val_loss: 23.2949\n",
            "Epoch 13/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 23.0019 - val_loss: 21.1708\n",
            "Epoch 14/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 20.9033 - val_loss: 19.1296\n",
            "Epoch 15/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 18.8876 - val_loss: 17.1696\n",
            "Epoch 16/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 16.9553 - val_loss: 15.2891\n",
            "Epoch 17/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 15.1045 - val_loss: 13.4855\n",
            "Epoch 18/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 13.3330 - val_loss: 11.7654\n",
            "Epoch 19/300\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 11.6435 - val_loss: 10.1367\n",
            "Epoch 20/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 10.0426 - val_loss: 8.6069\n",
            "Epoch 21/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8.5379 - val_loss: 7.1859\n",
            "Epoch 22/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7.1400 - val_loss: 5.8857\n",
            "Epoch 23/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.8607 - val_loss: 4.7181\n",
            "Epoch 24/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 4.7129 - val_loss: 3.6967\n",
            "Epoch 25/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 3.7103 - val_loss: 2.8339\n",
            "Epoch 26/300\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 2.8643 - val_loss: 2.1399\n",
            "Epoch 27/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.1859 - val_loss: 1.6239\n",
            "Epoch 28/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.6838 - val_loss: 1.2932\n",
            "Epoch 29/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.3629 - val_loss: 1.1460\n",
            "Epoch 30/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2219 - val_loss: 1.1650\n",
            "Epoch 31/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.2443 - val_loss: 1.3080\n",
            "Epoch 32/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.3872 - val_loss: 1.5185\n",
            "Epoch 33/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.5949 - val_loss: 1.7418\n",
            "Epoch 34/300\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.8144 - val_loss: 1.9367\n",
            "Epoch 35/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2.0053 - val_loss: 2.0774\n",
            "Epoch 36/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2.1419 - val_loss: 2.1520\n",
            "Epoch 37/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 2.2127 - val_loss: 2.1612\n",
            "Epoch 38/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.2179 - val_loss: 2.1131\n",
            "Epoch 39/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.1662 - val_loss: 2.0215\n",
            "Epoch 40/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.0725 - val_loss: 1.9013\n",
            "Epoch 41/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.9509 - val_loss: 1.7644\n",
            "Epoch 42/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.8129 - val_loss: 1.6227\n",
            "Epoch 43/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.6698 - val_loss: 1.4849\n",
            "Epoch 44/300\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.5308 - val_loss: 1.3581\n",
            "Epoch 45/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.4029 - val_loss: 1.2475\n",
            "Epoch 46/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2919 - val_loss: 1.1560\n",
            "Epoch 47/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.2006 - val_loss: 1.0848\n",
            "Epoch 48/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.1297 - val_loss: 1.0333\n",
            "Epoch 49/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.0785 - val_loss: 0.9994\n",
            "Epoch 50/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.0446 - val_loss: 0.9803\n",
            "Epoch 51/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0254 - val_loss: 0.9727\n",
            "Epoch 52/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.0176 - val_loss: 0.9731\n",
            "Epoch 53/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.0178 - val_loss: 0.9784\n",
            "Epoch 54/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.0226 - val_loss: 0.9853\n",
            "Epoch 55/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.0289 - val_loss: 0.9912\n",
            "Epoch 56/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.0340 - val_loss: 0.9937\n",
            "Epoch 57/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0356 - val_loss: 0.9921\n",
            "Epoch 58/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0328 - val_loss: 0.9861\n",
            "Epoch 59/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0261 - val_loss: 0.9760\n",
            "Epoch 60/300\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.0155 - val_loss: 0.9625\n",
            "Epoch 61/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0010 - val_loss: 0.9473\n",
            "Epoch 62/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9842 - val_loss: 0.9323\n",
            "Epoch 63/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.9681 - val_loss: 0.9204\n",
            "Epoch 64/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.9553 - val_loss: 0.9132\n",
            "Epoch 65/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9477 - val_loss: 0.9094\n",
            "Epoch 66/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.9436 - val_loss: 0.9054\n",
            "Epoch 67/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.9396 - val_loss: 0.8986\n",
            "Epoch 68/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.9330 - val_loss: 0.8882\n",
            "Epoch 69/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9231 - val_loss: 0.8755\n",
            "Epoch 70/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9106 - val_loss: 0.8618\n",
            "Epoch 71/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.8971 - val_loss: 0.8488\n",
            "Epoch 72/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.8843 - val_loss: 0.8373\n",
            "Epoch 73/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.8733 - val_loss: 0.8279\n",
            "Epoch 74/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.8641 - val_loss: 0.8199\n",
            "Epoch 75/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.8564 - val_loss: 0.8125\n",
            "Epoch 76/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.8493 - val_loss: 0.8049\n",
            "Epoch 77/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.8420 - val_loss: 0.7965\n",
            "Epoch 78/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.8341 - val_loss: 0.7873\n",
            "Epoch 79/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.8254 - val_loss: 0.7778\n",
            "Epoch 80/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.8163 - val_loss: 0.7683\n",
            "Epoch 81/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.8074 - val_loss: 0.7593\n",
            "Epoch 82/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7990 - val_loss: 0.7512\n",
            "Epoch 83/300\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7916 - val_loss: 0.7442\n",
            "Epoch 84/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7850 - val_loss: 0.7378\n",
            "Epoch 85/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7790 - val_loss: 0.7316\n",
            "Epoch 86/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7731 - val_loss: 0.7250\n",
            "Epoch 87/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7669 - val_loss: 0.7181\n",
            "Epoch 88/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7603 - val_loss: 0.7110\n",
            "Epoch 89/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7534 - val_loss: 0.7040\n",
            "Epoch 90/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7466 - val_loss: 0.6975\n",
            "Epoch 91/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7402 - val_loss: 0.6915\n",
            "Epoch 92/300\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7345 - val_loss: 0.6862\n",
            "Epoch 93/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7295 - val_loss: 0.6812\n",
            "Epoch 94/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7247 - val_loss: 0.6761\n",
            "Epoch 95/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7201 - val_loss: 0.6712\n",
            "Epoch 96/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7154 - val_loss: 0.6663\n",
            "Epoch 97/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7108 - val_loss: 0.6616\n",
            "Epoch 98/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7064 - val_loss: 0.6571\n",
            "Epoch 99/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7021 - val_loss: 0.6528\n",
            "Epoch 100/300\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6981 - val_loss: 0.6489\n",
            "Epoch 101/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6944 - val_loss: 0.6453\n",
            "Epoch 102/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6907 - val_loss: 0.6420\n",
            "Epoch 103/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6871 - val_loss: 0.6388\n",
            "Epoch 104/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6835 - val_loss: 0.6355\n",
            "Epoch 105/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6797 - val_loss: 0.6324\n",
            "Epoch 106/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6759 - val_loss: 0.6294\n",
            "Epoch 107/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6722 - val_loss: 0.6265\n",
            "Epoch 108/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6686 - val_loss: 0.6238\n",
            "Epoch 109/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6652 - val_loss: 0.6211\n",
            "Epoch 110/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6618 - val_loss: 0.6184\n",
            "Epoch 111/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6584 - val_loss: 0.6156\n",
            "Epoch 112/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6551 - val_loss: 0.6129\n",
            "Epoch 113/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6518 - val_loss: 0.6102\n",
            "Epoch 114/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6485 - val_loss: 0.6076\n",
            "Epoch 115/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6454 - val_loss: 0.6050\n",
            "Epoch 116/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6423 - val_loss: 0.6025\n",
            "Epoch 117/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6393 - val_loss: 0.6001\n",
            "Epoch 118/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6364 - val_loss: 0.5978\n",
            "Epoch 119/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6336 - val_loss: 0.5954\n",
            "Epoch 120/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6307 - val_loss: 0.5931\n",
            "Epoch 121/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6279 - val_loss: 0.5909\n",
            "Epoch 122/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6251 - val_loss: 0.5887\n",
            "Epoch 123/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6223 - val_loss: 0.5865\n",
            "Epoch 124/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6197 - val_loss: 0.5844\n",
            "Epoch 125/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6170 - val_loss: 0.5823\n",
            "Epoch 126/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6145 - val_loss: 0.5801\n",
            "Epoch 127/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6119 - val_loss: 0.5779\n",
            "Epoch 128/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6094 - val_loss: 0.5757\n",
            "Epoch 129/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6069 - val_loss: 0.5736\n",
            "Epoch 130/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6045 - val_loss: 0.5715\n",
            "Epoch 131/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6021 - val_loss: 0.5694\n",
            "Epoch 132/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5997 - val_loss: 0.5673\n",
            "Epoch 133/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5974 - val_loss: 0.5653\n",
            "Epoch 134/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5951 - val_loss: 0.5633\n",
            "Epoch 135/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5928 - val_loss: 0.5614\n",
            "Epoch 136/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5906 - val_loss: 0.5595\n",
            "Epoch 137/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5883 - val_loss: 0.5576\n",
            "Epoch 138/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5862 - val_loss: 0.5557\n",
            "Epoch 139/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5840 - val_loss: 0.5538\n",
            "Epoch 140/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.5818 - val_loss: 0.5520\n",
            "Epoch 141/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.5797 - val_loss: 0.5501\n",
            "Epoch 142/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5776 - val_loss: 0.5483\n",
            "Epoch 143/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.5755 - val_loss: 0.5464\n",
            "Epoch 144/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5735 - val_loss: 0.5446\n",
            "Epoch 145/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5715 - val_loss: 0.5429\n",
            "Epoch 146/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5694 - val_loss: 0.5411\n",
            "Epoch 147/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5674 - val_loss: 0.5393\n",
            "Epoch 148/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5655 - val_loss: 0.5376\n",
            "Epoch 149/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5635 - val_loss: 0.5359\n",
            "Epoch 150/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.5615 - val_loss: 0.5341\n",
            "Epoch 151/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5596 - val_loss: 0.5324\n",
            "Epoch 152/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5577 - val_loss: 0.5307\n",
            "Epoch 153/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5558 - val_loss: 0.5291\n",
            "Epoch 154/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5539 - val_loss: 0.5274\n",
            "Epoch 155/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5521 - val_loss: 0.5257\n",
            "Epoch 156/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5502 - val_loss: 0.5241\n",
            "Epoch 157/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5484 - val_loss: 0.5225\n",
            "Epoch 158/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5466 - val_loss: 0.5208\n",
            "Epoch 159/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5448 - val_loss: 0.5192\n",
            "Epoch 160/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.5430 - val_loss: 0.5176\n",
            "Epoch 161/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5413 - val_loss: 0.5161\n",
            "Epoch 162/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5396 - val_loss: 0.5146\n",
            "Epoch 163/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5379 - val_loss: 0.5130\n",
            "Epoch 164/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5362 - val_loss: 0.5116\n",
            "Epoch 165/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.5346 - val_loss: 0.5101\n",
            "Epoch 166/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5329 - val_loss: 0.5087\n",
            "Epoch 167/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.5313 - val_loss: 0.5072\n",
            "Epoch 168/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5298 - val_loss: 0.5058\n",
            "Epoch 169/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5282 - val_loss: 0.5044\n",
            "Epoch 170/300\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.5267 - val_loss: 0.5030\n",
            "Epoch 171/300\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 0.5251 - val_loss: 0.5017\n",
            "Epoch 172/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5236 - val_loss: 0.5003\n",
            "Epoch 173/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5222 - val_loss: 0.4989\n",
            "Epoch 174/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5207 - val_loss: 0.4976\n",
            "Epoch 175/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5192 - val_loss: 0.4962\n",
            "Epoch 176/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5178 - val_loss: 0.4949\n",
            "Epoch 177/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5163 - val_loss: 0.4936\n",
            "Epoch 178/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5149 - val_loss: 0.4922\n",
            "Epoch 179/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5134 - val_loss: 0.4909\n",
            "Epoch 180/300\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.5120 - val_loss: 0.4896\n",
            "Epoch 181/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5106 - val_loss: 0.4882\n",
            "Epoch 182/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5091 - val_loss: 0.4869\n",
            "Epoch 183/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5077 - val_loss: 0.4855\n",
            "Epoch 184/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5063 - val_loss: 0.4842\n",
            "Epoch 185/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5048 - val_loss: 0.4829\n",
            "Epoch 186/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5034 - val_loss: 0.4815\n",
            "Epoch 187/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5020 - val_loss: 0.4802\n",
            "Epoch 188/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5005 - val_loss: 0.4788\n",
            "Epoch 189/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4991 - val_loss: 0.4775\n",
            "Epoch 190/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4976 - val_loss: 0.4762\n",
            "Epoch 191/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4962 - val_loss: 0.4749\n",
            "Epoch 192/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4947 - val_loss: 0.4736\n",
            "Epoch 193/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4933 - val_loss: 0.4723\n",
            "Epoch 194/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.4918 - val_loss: 0.4710\n",
            "Epoch 195/300\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4904 - val_loss: 0.4697\n",
            "Epoch 196/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4889 - val_loss: 0.4683\n",
            "Epoch 197/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4874 - val_loss: 0.4670\n",
            "Epoch 198/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4859 - val_loss: 0.4657\n",
            "Epoch 199/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4844 - val_loss: 0.4643\n",
            "Epoch 200/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4829 - val_loss: 0.4630\n",
            "Epoch 201/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4814 - val_loss: 0.4617\n",
            "Epoch 202/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4799 - val_loss: 0.4603\n",
            "Epoch 203/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4784 - val_loss: 0.4590\n",
            "Epoch 204/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4769 - val_loss: 0.4576\n",
            "Epoch 205/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4753 - val_loss: 0.4563\n",
            "Epoch 206/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4738 - val_loss: 0.4549\n",
            "Epoch 207/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.4723 - val_loss: 0.4536\n",
            "Epoch 208/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4707 - val_loss: 0.4522\n",
            "Epoch 209/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4692 - val_loss: 0.4508\n",
            "Epoch 210/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4677 - val_loss: 0.4494\n",
            "Epoch 211/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4661 - val_loss: 0.4480\n",
            "Epoch 212/300\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4646 - val_loss: 0.4466\n",
            "Epoch 213/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4631 - val_loss: 0.4452\n",
            "Epoch 214/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4615 - val_loss: 0.4437\n",
            "Epoch 215/300\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4600 - val_loss: 0.4423\n",
            "Epoch 216/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4584 - val_loss: 0.4409\n",
            "Epoch 217/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4568 - val_loss: 0.4394\n",
            "Epoch 218/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4552 - val_loss: 0.4380\n",
            "Epoch 219/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4536 - val_loss: 0.4365\n",
            "Epoch 220/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4520 - val_loss: 0.4350\n",
            "Epoch 221/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4504 - val_loss: 0.4336\n",
            "Epoch 222/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.4488 - val_loss: 0.4321\n",
            "Epoch 223/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4472 - val_loss: 0.4308\n",
            "Epoch 224/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4456 - val_loss: 0.4294\n",
            "Epoch 225/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4440 - val_loss: 0.4279\n",
            "Epoch 226/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4424 - val_loss: 0.4264\n",
            "Epoch 227/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4407 - val_loss: 0.4249\n",
            "Epoch 228/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4391 - val_loss: 0.4234\n",
            "Epoch 229/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4374 - val_loss: 0.4219\n",
            "Epoch 230/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4358 - val_loss: 0.4204\n",
            "Epoch 231/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4341 - val_loss: 0.4188\n",
            "Epoch 232/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4324 - val_loss: 0.4173\n",
            "Epoch 233/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4307 - val_loss: 0.4157\n",
            "Epoch 234/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4290 - val_loss: 0.4141\n",
            "Epoch 235/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4273 - val_loss: 0.4125\n",
            "Epoch 236/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4256 - val_loss: 0.4108\n",
            "Epoch 237/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4239 - val_loss: 0.4092\n",
            "Epoch 238/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4222 - val_loss: 0.4075\n",
            "Epoch 239/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4205 - val_loss: 0.4059\n",
            "Epoch 240/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4188 - val_loss: 0.4042\n",
            "Epoch 241/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4170 - val_loss: 0.4026\n",
            "Epoch 242/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4153 - val_loss: 0.4010\n",
            "Epoch 243/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4136 - val_loss: 0.3994\n",
            "Epoch 244/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4118 - val_loss: 0.3978\n",
            "Epoch 245/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4101 - val_loss: 0.3962\n",
            "Epoch 246/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4084 - val_loss: 0.3946\n",
            "Epoch 247/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4066 - val_loss: 0.3929\n",
            "Epoch 248/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4048 - val_loss: 0.3912\n",
            "Epoch 249/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4031 - val_loss: 0.3895\n",
            "Epoch 250/300\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4013 - val_loss: 0.3878\n",
            "Epoch 251/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3995 - val_loss: 0.3861\n",
            "Epoch 252/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3977 - val_loss: 0.3845\n",
            "Epoch 253/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3959 - val_loss: 0.3828\n",
            "Epoch 254/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3942 - val_loss: 0.3811\n",
            "Epoch 255/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3924 - val_loss: 0.3795\n",
            "Epoch 256/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3906 - val_loss: 0.3778\n",
            "Epoch 257/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3889 - val_loss: 0.3762\n",
            "Epoch 258/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3871 - val_loss: 0.3745\n",
            "Epoch 259/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3854 - val_loss: 0.3729\n",
            "Epoch 260/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3837 - val_loss: 0.3712\n",
            "Epoch 261/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.3820 - val_loss: 0.3695\n",
            "Epoch 262/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3804 - val_loss: 0.3679\n",
            "Epoch 263/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3787 - val_loss: 0.3663\n",
            "Epoch 264/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3770 - val_loss: 0.3647\n",
            "Epoch 265/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3753 - val_loss: 0.3631\n",
            "Epoch 266/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.3737 - val_loss: 0.3615\n",
            "Epoch 267/300\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3720 - val_loss: 0.3599\n",
            "Epoch 268/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3704 - val_loss: 0.3584\n",
            "Epoch 269/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3688 - val_loss: 0.3568\n",
            "Epoch 270/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3672 - val_loss: 0.3553\n",
            "Epoch 271/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3655 - val_loss: 0.3538\n",
            "Epoch 272/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3639 - val_loss: 0.3523\n",
            "Epoch 273/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3623 - val_loss: 0.3507\n",
            "Epoch 274/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3607 - val_loss: 0.3492\n",
            "Epoch 275/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3591 - val_loss: 0.3477\n",
            "Epoch 276/300\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.3575 - val_loss: 0.3462\n",
            "Epoch 277/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3560 - val_loss: 0.3447\n",
            "Epoch 278/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3544 - val_loss: 0.3433\n",
            "Epoch 279/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3528 - val_loss: 0.3418\n",
            "Epoch 280/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3512 - val_loss: 0.3403\n",
            "Epoch 281/300\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3497 - val_loss: 0.3389\n",
            "Epoch 282/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3482 - val_loss: 0.3375\n",
            "Epoch 283/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3466 - val_loss: 0.3361\n",
            "Epoch 284/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3451 - val_loss: 0.3347\n",
            "Epoch 285/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3436 - val_loss: 0.3334\n",
            "Epoch 286/300\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.3421 - val_loss: 0.3320\n",
            "Epoch 287/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3406 - val_loss: 0.3306\n",
            "Epoch 288/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3391 - val_loss: 0.3293\n",
            "Epoch 289/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.3376 - val_loss: 0.3280\n",
            "Epoch 290/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3361 - val_loss: 0.3266\n",
            "Epoch 291/300\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.3346 - val_loss: 0.3253\n",
            "Epoch 292/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3331 - val_loss: 0.3240\n",
            "Epoch 293/300\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.3316 - val_loss: 0.3227\n",
            "Epoch 294/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3302 - val_loss: 0.3214\n",
            "Epoch 295/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3287 - val_loss: 0.3201\n",
            "Epoch 296/300\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3272 - val_loss: 0.3189\n",
            "Epoch 297/300\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3258 - val_loss: 0.3176\n",
            "Epoch 298/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3244 - val_loss: 0.3162\n",
            "Epoch 299/300\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3229 - val_loss: 0.3149\n",
            "Epoch 300/300\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3215 - val_loss: 0.3136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.8293612941756132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtzTLUaCorXO",
        "colab_type": "text"
      },
      "source": [
        "**Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne_URAzMQy86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0384f23d-9576-4825-c7e8-2812616faab9"
      },
      "source": [
        "RFmodel = RandomForestRegressor(n_estimators=900)\n",
        "RFmodel.fit(X_train2,y_train2) # making random forest regressor\n",
        "predict = RFmodel.predict(X_test2) \n",
        "sklearn.metrics.r2_score(y_test2,RFmodel.predict(X_test2)) # r2 score for random forest regressor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8881161838243156"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4sXFqcupLeb",
        "colab_type": "text"
      },
      "source": [
        "**Using our models to make a Custom Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPm4RiIQ2_Gd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2201d94c-b3c4-45e0-a698-232914668fe4"
      },
      "source": [
        "stri = input() # input your custom grades \n",
        "arr = to_arr(stri)\n",
        "RFmodel.predict([arr])[0] # making our first custom prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "B+ B+ A B+ B+ A\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.269444444444513"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    }
  ]
}